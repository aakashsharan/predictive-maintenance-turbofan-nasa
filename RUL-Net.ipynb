{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RUL-Net.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyORLpAWJyC7fM3tZUfJ/2Ek"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Reference: https://github.com/LahiruJayasinghe/RUL-Net"],"metadata":{"id":"VvSi5_IqyNMQ"}},{"cell_type":"code","source":["!pip install tensorflow==1.15.5"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5RL-L5Hh0TxW","executionInfo":{"status":"ok","timestamp":1643956499918,"user_tz":480,"elapsed":5025,"user":{"displayName":"Yoshifumi Suzuki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhA5ySxPD4d5EAxHB7_YxB1GzsFPXBv17bpN1w-uUA=s64","userId":"11502972587532747475"}},"outputId":"79f7d64e-f101-4848-f6b8-af35502e83ff"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","Requirement already satisfied: tensorflow==1.15.5 in /usr/local/lib/python3.7/dist-packages (1.15.5)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (3.17.3)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.1.2)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (0.37.1)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.15.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.0.0)\n","Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (0.2.2)\n","Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.18.5)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.1.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.13.3)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (0.2.0)\n","Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.15.0)\n","Requirement already satisfied: h5py<=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (2.10.0)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.43.0)\n","Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.15.1)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (0.8.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (3.3.0)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.0.8)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (1.0.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (57.4.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (3.3.6)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (4.10.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (3.7.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (3.10.0.2)\n","\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n"]}]},{"cell_type":"code","source":["!pip install \"scikit-learn==0.19.1\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CMgfk1GL1Hrd","executionInfo":{"status":"ok","timestamp":1643956528214,"user_tz":480,"elapsed":20519,"user":{"displayName":"Yoshifumi Suzuki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhA5ySxPD4d5EAxHB7_YxB1GzsFPXBv17bpN1w-uUA=s64","userId":"11502972587532747475"}},"outputId":"333c09b3-d2e5-4e21-c631-9590fcc0d12c"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","Collecting scikit-learn==0.19.1\n","  Downloading scikit-learn-0.19.1.tar.gz (9.5 MB)\n","\u001b[K     |████████████████████████████████| 9.5 MB 12.0 MB/s \n","\u001b[?25hBuilding wheels for collected packages: scikit-learn\n","  Building wheel for scikit-learn (setup.py) ... \u001b[?25lerror\n","\u001b[31m  ERROR: Failed building wheel for scikit-learn\u001b[0m\n","\u001b[?25h  Running setup.py clean for scikit-learn\n","Failed to build scikit-learn\n","\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","Installing collected packages: scikit-learn\n","  Attempting uninstall: scikit-learn\n","\u001b[33m    WARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","    Found existing installation: scikit-learn 1.0.2\n","    Uninstalling scikit-learn-1.0.2:\n","      Successfully uninstalled scikit-learn-1.0.2\n","    Running setup.py install for scikit-learn ... \u001b[?25l\u001b[?25herror\n","  Rolling back uninstall of scikit-learn\n","  Moving to /usr/local/lib/python3.7/dist-packages/scikit_learn-1.0.2.dist-info/\n","   from /usr/local/lib/python3.7/dist-packages/~cikit_learn-1.0.2.dist-info\n","  Moving to /usr/local/lib/python3.7/dist-packages/scikit_learn.libs/\n","   from /usr/local/lib/python3.7/dist-packages/~cikit_learn.libs\n","  Moving to /usr/local/lib/python3.7/dist-packages/sklearn/\n","   from /usr/local/lib/python3.7/dist-packages/~klearn\n","\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-rc8vvl65/scikit-learn_fc9b12bdae13469bb821f04c88e8721a/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-rc8vvl65/scikit-learn_fc9b12bdae13469bb821f04c88e8721a/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-hu391ac4/install-record.txt --single-version-externally-managed --compile --install-headers /usr/local/include/python3.7/scikit-learn Check the logs for full command output.\u001b[0m\n"]}]},{"cell_type":"code","source":["!pip install process-data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bVG-Dr_01YYT","executionInfo":{"status":"ok","timestamp":1643956547437,"user_tz":480,"elapsed":3280,"user":{"displayName":"Yoshifumi Suzuki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhA5ySxPD4d5EAxHB7_YxB1GzsFPXBv17bpN1w-uUA=s64","userId":"11502972587532747475"}},"outputId":"4700b43a-8550-491f-8f0b-d36604027ea8"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","Collecting process-data\n","  Downloading process_data-0.3-py3-none-any.whl (3.5 kB)\n","\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","Installing collected packages: process-data\n","\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","Successfully installed process-data-0.3\n"]}]},{"cell_type":"markdown","source":["## utils_laj.py"],"metadata":{"id":"xd2fTzIC3THj"}},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5dXehYNKyMAS","executionInfo":{"status":"ok","timestamp":1643956557564,"user_tz":480,"elapsed":6026,"user":{"displayName":"Yoshifumi Suzuki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhA5ySxPD4d5EAxHB7_YxB1GzsFPXBv17bpN1w-uUA=s64","userId":"11502972587532747475"}},"outputId":"0eda9d34-a9c8-4b78-fe4e-be5317aa0ece"},"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n"]}],"source":["import os\n","import numpy as np\n","import tensorflow as tf\n","import tensorflow.contrib.slim as slim\n","# from data_processing import MAXLIFE"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import os\n","os.chdir('/content/drive/My Drive/Colab Notebooks/fourthbrain/capstone_code/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XP68DpCizh_6","executionInfo":{"status":"ok","timestamp":1643956580681,"user_tz":480,"elapsed":20349,"user":{"displayName":"Yoshifumi Suzuki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhA5ySxPD4d5EAxHB7_YxB1GzsFPXBv17bpN1w-uUA=s64","userId":"11502972587532747475"}},"outputId":"8b8d2442-f6f5-43f7-cac3-16e0d32df3bf"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["def dense_layer(x, size,activation_fn, batch_norm = False,phase=False, drop_out=False, keep_prob=None, scope=\"fc_layer\"):\n","    \"\"\"\n","    Helper function to create a fully connected layer with or without batch normalization or dropout regularization\n","\n","    :param x: previous layer\n","    :param size: fully connected layer size\n","    :param activation_fn: activation function\n","    :param batch_norm: bool to set batch normalization\n","    :param phase: if batch normalization is set, then phase variable is to mention the 'training' and 'testing' phases\n","    :param drop_out: bool to set drop-out regularization\n","    :param keep_prob: if drop-out is set, then to mention the keep probability of dropout\n","    :param scope: variable scope name\n","    :return: fully connected layer\n","    \"\"\"\n","    with tf.variable_scope(scope):\n","        if batch_norm:\n","            dence_layer = tf.contrib.layers.fully_connected(x, size, activation_fn=None)\n","            dence_layer_bn = BatchNorm(name=\"batch_norm_\" + scope)(dence_layer, train=phase)\n","            return_layer = activation_fn(dence_layer_bn)\n","        else:\n","            return_layer = tf.layers.dense(x, size,\n","                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n","                                           activation=activation_fn)\n","        if drop_out:\n","            return_layer = tf.nn.dropout(return_layer, keep_prob)\n","\n","        return return_layer\n","\n","\n","def get_RNNCell(cell_types, keep_prob, state_size, build_with_dropout=True):\n","    \"\"\"\n","    Helper function to get a different types of RNN cells with or without dropout wrapper\n","    :param cell_types: cell_type can be 'GRU' or 'LSTM' or 'LSTM_LN' or 'GLSTMCell' or 'LSTM_BF' or 'None'\n","    :param keep_prob: dropout keeping probability\n","    :param state_size: number of cells in a layer\n","    :param build_with_dropout: to enable the dropout for rnn layers\n","    :return:\n","    \"\"\"\n","    cells = []\n","    for cell_type in cell_types:\n","        if cell_type == 'GRU':\n","            cell = tf.contrib.rnn.GRUCell(num_units=state_size,\n","                                          bias_initializer=tf.zeros_initializer())  # Or GRU(num_units)\n","        elif cell_type == 'LSTM':\n","            cell = tf.contrib.rnn.LSTMCell(num_units=state_size, use_peepholes=True, state_is_tuple=True,\n","                                           initializer=tf.contrib.layers.xavier_initializer())\n","        elif cell_type == 'LSTM_LN':\n","            cell = tf.contrib.rnn.LayerNormBasicLSTMCell(state_size)\n","        elif cell_type == 'GLSTMCell':\n","            cell = tf.contrib.rnn.GLSTMCell(num_units=state_size, initializer=tf.contrib.layers.xavier_initializer())\n","        elif cell_type == 'LSTM_BF':\n","            cell = tf.contrib.rnn.LSTMBlockFusedCell(num_units=state_size, use_peephole=True)\n","        else:\n","            cell = tf.nn.rnn_cell.BasicRNNCell(state_size)\n","\n","        if build_with_dropout:\n","            cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\n","        cells.append(cell)\n","\n","    cell = tf.contrib.rnn.MultiRNNCell(cells)\n","\n","    if build_with_dropout:\n","        cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\n","\n","    return cell\n","\n","\n","class BatchNorm(object):\n","    \"\"\"\n","    usage : dence_layer_bn = BatchNorm(name=\"batch_norm_\" + scope)(previous_layer, train=is_train)\n","    \"\"\"\n","    def __init__(self, epsilon=1e-5, momentum=0.999, name=\"batch_norm\"):\n","        with tf.variable_scope(name):\n","            self.epsilon = epsilon\n","            self.momentum = momentum\n","            self.name = name\n","\n","    def __call__(self, x, train=True):\n","        return tf.contrib.layers.batch_norm(x,\n","                                            decay=self.momentum,\n","                                            updates_collections=None,\n","                                            epsilon=self.epsilon,\n","                                            scale=True,\n","                                            is_training=train,\n","                                            scope=self.name)\n","\n","\n","def batch_generator(x_train, y_train, batch_size, sequence_length, online=False, online_shift=1):\n","    \"\"\"\n","    Generator function for creating random batches of training-data for many to many models\n","    \"\"\"\n","    num_x_sensors = x_train.shape[1]\n","    num_train = x_train.shape[0]\n","    idx = 0\n","\n","    # Infinite loop.\n","    while True:\n","        # Allocate a new array for the batch of input-signals.\n","        x_shape = (batch_size, sequence_length, num_x_sensors)\n","        x_batch = np.zeros(shape=x_shape, dtype=np.float32)\n","        # print(idx)\n","        # Allocate a new array for the batch of output-signals.\n","        y_shape = (batch_size, sequence_length)\n","        y_batch = np.zeros(shape=y_shape, dtype=np.float32)\n","\n","        # Fill the batch with random sequences of data.\n","        for i in range(batch_size):\n","            # Get a random start-index.\n","            # This points somewhere into the training-data.\n","            if online == True and (idx >= num_train or (idx + sequence_length) > num_train):\n","                idx = 0\n","            elif online == False:\n","                idx = np.random.randint(num_train - sequence_length)\n","\n","            # Copy the sequences of data starting at this index.\n","            x_batch[i] = x_train[idx:idx + sequence_length]\n","            y_batch[i] = y_train[idx:idx + sequence_length]\n","            # print(i,idx)\n","            if online:\n","                idx = idx + online_shift  # check if its nee to be idx=idx+1\n","                # print(idx)\n","        # print(idx)\n","        yield (x_batch, y_batch)\n","\n","\n","def trjectory_generator(x_train, y_train, test_engine_id, sequence_length, graph_batch_size, lower_bound):\n","    \"\"\"\n","    Extract training trjectories one by one\n","    test_engine_id = [11111111...,22222222....,...]\n","    \"\"\"\n","    DEBUG = False\n","    num_x_sensors = x_train.shape[1]\n","    idx = 0\n","    engine_ids = test_engine_id.unique()\n","    if DEBUG: print(\"total trjectories: \", len(engine_ids))\n","\n","    while True:\n","        for id in engine_ids:\n","\n","            indexes = test_engine_id[test_engine_id == id].index\n","            training_data = x_train[indexes]\n","            if DEBUG: print(\"engine_id: \", id, \"start\", indexes[0], \"end\", indexes[-1], \"trjectory_len:\", len(indexes))\n","            batch_size = int(training_data.shape[0] / sequence_length) + 1\n","            idx = indexes[0]\n","\n","            x_batch = np.zeros(shape=(batch_size, sequence_length, num_x_sensors), dtype=np.float32)\n","            y_batch = np.zeros(shape=(batch_size, sequence_length), dtype=np.float32)\n","\n","            for i in range(batch_size):\n","\n","                # Copy the sequences of data starting at this index.\n","                if DEBUG: print(\"current idx=\", idx)\n","                if idx >= x_train.shape[0]:\n","                    if DEBUG: print(\"BREAK\")\n","                    break\n","                elif (idx + sequence_length) > x_train.shape[0]:\n","                    if DEBUG: print(\"BREAK\", idx, x_train.shape[0], idx + sequence_length - x_train.shape[0])\n","                    x_tmp = x_train[idx:]\n","                    y_tmp = y_train[idx:]\n","                    remain = idx + sequence_length - x_train.shape[0]\n","                    x_batch[i] = np.concatenate((x_tmp, x_train[0:remain]))\n","                    y_batch[i] = np.concatenate((y_tmp, y_train[0:remain]))\n","                    break\n","\n","                x_batch[i] = x_train[idx:idx + sequence_length]\n","\n","                if idx > indexes[-1] - sequence_length:\n","                    y_tmp = np.copy(y_train[idx:idx + sequence_length])\n","                    remain = sequence_length - (indexes[-1] - idx + 1)  # abs(training_data.shape[0]-sequence_length)\n","                    if DEBUG: print(\"(idx + sequence_length) > trj_len:\", \"remain\", remain)\n","                    y_tmp[-remain:] = lower_bound\n","                    y_batch[i] = y_tmp\n","                else:\n","                    y_batch[i] = y_train[idx:idx + sequence_length]\n","\n","                idx = idx + sequence_length\n","\n","            batch_size_gap = graph_batch_size - x_batch.shape[0]\n","            if batch_size_gap > 0:\n","                for i in range(batch_size_gap):\n","                    x_tmp = -0.01 * np.ones(shape=(sequence_length, num_x_sensors), dtype=np.float32)\n","                    y_tmp = -0.01 * np.ones(shape=(sequence_length), dtype=np.float32)\n","                    xx = np.append(x_batch, x_tmp)\n","                    x_batch = np.reshape(xx, [x_batch.shape[0] + 1, x_batch.shape[1], x_batch.shape[2]])\n","                    yy = np.append(y_batch, y_tmp)\n","                    y_batch = np.reshape(yy, [y_batch.shape[0] + 1, x_batch.shape[1]])\n","            yield (x_batch, y_batch)\n","\n","\n","def plot_data(data, label=\"\"):\n","    \"\"\"\n","    Plot every plot on top of each other\n","    \"\"\"\n","    from matplotlib import pyplot as plt\n","    if type(data) is list:\n","        for x in data:\n","            plt.plot(x, label=label)\n","    else:\n","        plt.plot(data, label=label)\n","    plt.show()\n","\n","\n","def model_summary(learning_rate,batch_size,lstm_layers,lstm_layer_size,fc_layer_size,sequence_length,n_channels,path_checkpoint,spacial_note=''):\n","    path_checkpoint=path_checkpoint + \".txt\"\n","    if not os.path.exists(os.path.dirname(path_checkpoint)):\n","        os.makedirs(os.path.dirname(path_checkpoint))\n","\n","    with open(path_checkpoint, \"w\") as text_file:\n","        variables = tf.trainable_variables()\n","\n","        print('---------', file=text_file)\n","        print(path_checkpoint, file=text_file)\n","        print(spacial_note, file=text_file)\n","        print('---------', '\\n', file=text_file)\n","\n","        print('---------', file=text_file)\n","        print('MAXLIFE: ', MAXLIFE,'\\n',  file=text_file)\n","        print('learning_rate: ', learning_rate, file=text_file)\n","        print('batch_size: ', batch_size, file=text_file)\n","        print('lstm_layers: ', lstm_layers, file=text_file)\n","        print('lstm_layer_size: ', lstm_layer_size, file=text_file)\n","        print('fc_layer_size: ', fc_layer_size, '\\n', file=text_file)\n","        print('sequence_length: ', sequence_length, file=text_file)\n","        print('n_channels: ', n_channels, file=text_file)\n","        print('---------', '\\n', file=text_file)\n","\n","        print('---------', file=text_file)\n","        print('Variables: name (type shape) [size]', file=text_file)\n","        print('---------', '\\n', file=text_file)\n","        total_size = 0\n","        total_bytes = 0\n","        for var in variables:\n","            # if var.num_elements() is None or [] assume size 0.\n","            var_size = var.get_shape().num_elements() or 0\n","            var_bytes = var_size * var.dtype.size\n","            total_size += var_size\n","            total_bytes += var_bytes\n","            print(var.name, slim.model_analyzer.tensor_description(var), '[%d, bytes: %d]' %\n","                      (var_size, var_bytes), file=text_file)\n","\n","        print('\\nTotal size of variables: %d' % total_size, file=text_file)\n","        print('Total bytes of variables: %d' % total_bytes, file=text_file)\n","\n","\n","def scoring_func(error_arr):\n","    '''\n","\n","    :param error_arr: a list of errors for each training trajectory\n","    :return: standered score value for RUL\n","    '''\n","    import math\n","    # print(error_arr)\n","    pos_error_arr = error_arr[error_arr >= 0]\n","    neg_error_arr = error_arr[error_arr < 0]\n","\n","    score = 0\n","    # print(neg_error_arr)\n","    for error in neg_error_arr:\n","        score = math.exp(-(error / 13)) - 1 + score\n","        # print(math.exp(-(error / 13)),score,error)\n","\n","    # print(pos_error_arr)\n","    for error in pos_error_arr:\n","        score = math.exp(error / 10) - 1 + score\n","        # print(math.exp(error / 10),score, error)\n","    return score\n","\n","\n","def conv_layer(X,filters,kernel_size,strides,padding,batch_norm,is_train,scope):\n","    \"\"\"\n","    1D convolutional layer with or without dropout or batch normalization\n","\n","    :param batch_norm:  bool, enable batch normalization\n","    :param is_train: bool, mention if current phase is training phase\n","    :param scope: variable scope\n","    :return: 1D-convolutional layer\n","    \"\"\"\n","    with tf.variable_scope(scope):\n","        if batch_norm:\n","            conv1 = tf.layers.conv1d(inputs=X, filters=filters, kernel_size=kernel_size, strides=strides,\n","                                     padding=padding, kernel_initializer=tf.contrib.layers.xavier_initializer())\n","            return tf.nn.relu(BatchNorm(name=\"norm_\"+scope)(conv1, train=is_train))\n","        else:\n","            return tf.layers.conv1d(inputs=X, filters=filters, kernel_size=kernel_size, strides=strides,\n","                                     padding=padding, kernel_initializer=tf.contrib.layers.xavier_initializer(),\n","                                     activation=tf.nn.relu)\n","\n","\n","def get_predicted_expected_RUL(__y, __y_pred, lower_bound=-1):\n","    trj_end = np.argmax(__y == lower_bound) - 1\n","    trj_pred = __y_pred[:trj_end]\n","    trj_pred[trj_pred < 0] = 0\n","    # if trj_pred[-1] < 0: print(trj_pred[-1])\n","    RUL_predict = round(trj_pred[-1], 0)\n","    RUL_expected = round(__y[trj_end], 0)\n","\n","    return RUL_predict, RUL_expected\n"],"metadata":{"id":"mQb93yGPz67R","executionInfo":{"status":"ok","timestamp":1643956584056,"user_tz":480,"elapsed":1002,"user":{"displayName":"Yoshifumi Suzuki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhA5ySxPD4d5EAxHB7_YxB1GzsFPXBv17bpN1w-uUA=s64","userId":"11502972587532747475"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["## data_processing.py"],"metadata":{"id":"TZwF3lnq3YEQ"}},{"cell_type":"code","source":["import pandas as pd\n","from matplotlib import pyplot as plt\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","import random\n","\n","MAXLIFE = 120\n","SCALE = 1\n","RESCALE = 1\n","true_rul = []\n","test_engine_id = 0\n","training_engine_id = 0"],"metadata":{"id":"FkeGOzjC3gOJ","executionInfo":{"status":"ok","timestamp":1643956588971,"user_tz":480,"elapsed":255,"user":{"displayName":"Yoshifumi Suzuki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhA5ySxPD4d5EAxHB7_YxB1GzsFPXBv17bpN1w-uUA=s64","userId":"11502972587532747475"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def kink_RUL(cycle_list, max_cycle):\n","    '''\n","    Piecewise linear function with zero gradient and unit gradient\n","\n","            ^\n","            |\n","    MAXLIFE |-----------\n","            |            \\\n","            |             \\\n","            |              \\\n","            |               \\\n","            |                \\\n","            |----------------------->\n","    '''\n","    knee_point = max_cycle - MAXLIFE\n","    kink_RUL = []\n","    stable_life = MAXLIFE\n","    for i in range(0, len(cycle_list)):\n","        if i < knee_point:\n","            kink_RUL.append(MAXLIFE)\n","        else:\n","            tmp = kink_RUL[i - 1] - (stable_life / (max_cycle - knee_point))\n","            kink_RUL.append(tmp)\n","\n","    return kink_RUL\n","\n","\n","def compute_rul_of_one_id(FD00X_of_one_id, max_cycle_rul=None):\n","    '''\n","    Enter the data of an engine_id of train_FD001 and output the corresponding RUL (remaining life) of these data.\n","    type is list\n","    '''\n","\n","    cycle_list = FD00X_of_one_id['cycle'].tolist()\n","    if max_cycle_rul is None:\n","        max_cycle = max(cycle_list)  # Failure cycle\n","    else:\n","        max_cycle = max(cycle_list) + max_cycle_rul\n","        # print(max(cycle_list), max_cycle_rul)\n","\n","    # return kink_RUL(cycle_list,max_cycle)\n","    return kink_RUL(cycle_list, max_cycle)\n","\n","\n","def compute_rul_of_one_file(FD00X, id='engine_id', RUL_FD00X=None):\n","    '''\n","    Input train_FD001, output a list\n","    '''\n","    rul = []\n","    # In the loop train, each id value of the 'engine_id' column\n","    if RUL_FD00X is None:\n","        for _id in set(FD00X[id]):\n","            rul.extend(compute_rul_of_one_id(FD00X[FD00X[id] == _id]))\n","        return rul\n","    else:\n","        rul = []\n","        for _id in set(FD00X[id]):\n","            # print(\"#### id ####\", int(RUL_FD00X.iloc[_id - 1]))\n","            true_rul.append(int(RUL_FD00X.iloc[_id - 1]))\n","            rul.extend(compute_rul_of_one_id(FD00X[FD00X[id] == _id], int(RUL_FD00X.iloc[_id - 1])))\n","        return rul\n","\n","\n","def get_CMAPSSData(save=False, save_training_data=True, save_testing_data=True, files=[1, 2, 3, 4, 5],\n","                   min_max_norm=False):\n","    '''\n","    :param save: switch to load the already preprocessed data or begin preprocessing of raw data\n","    :param save_training_data: same functionality as 'save' but for training data only\n","    :param save_testing_data: same functionality as 'save' but for testing data only\n","    :param files: to indicate which sub dataset needed to be loaded for operations\n","    :param min_max_norm: switch to enable min-max normalization\n","    :return: function will save the preprocessed training and testing data as numpy objects\n","    '''\n","\n","    if save == False:\n","        return np.load(\"normalized_train_data.npy\"), np.load(\"normalized_test_data.npy\"), pd.read_csv(\n","            'normalized_train_data.csv', index_col=[0]), pd.read_csv('normalized_test_data.csv', index_col=[0])\n","\n","    column_name = ['engine_id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n","                   's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n","                   's15', 's16', 's17', 's18', 's19', 's20', 's21']\n","\n","    if save_training_data:  ### Training ###\n","\n","        train_FD001 = pd.read_table(\"./CMAPSSData/train_FD001.txt\", header=None, delim_whitespace=True)\n","        train_FD002 = pd.read_table(\"./CMAPSSData/train_FD002.txt\", header=None, delim_whitespace=True)\n","        train_FD003 = pd.read_table(\"./CMAPSSData/train_FD003.txt\", header=None, delim_whitespace=True)\n","        train_FD004 = pd.read_table(\"./CMAPSSData/train_FD004.txt\", header=None, delim_whitespace=True)\n","        train_FD001.columns = column_name\n","        train_FD002.columns = column_name\n","        train_FD003.columns = column_name\n","        train_FD004.columns = column_name\n","\n","        previous_len = 0\n","        frames = []\n","        for data_file in ['train_FD00' + str(i) for i in files]:  # load subdataset by subdataset\n","\n","            #### standard normalization ####\n","            mean = eval(data_file).iloc[:, 2:len(list(eval(data_file)))].mean()\n","            std = eval(data_file).iloc[:, 2:len(list(eval(data_file)))].std()\n","            std.replace(0, 1, inplace=True)\n","            # print(\"std\", std)\n","            ################################\n","\n","            if min_max_norm:\n","                scaler = MinMaxScaler()\n","                eval(data_file).iloc[:, 2:len(list(eval(data_file)))] = scaler.fit_transform(\n","                    eval(data_file).iloc[:, 2:len(list(eval(data_file)))])\n","            else:\n","                eval(data_file).iloc[:, 2:len(list(eval(data_file)))] = (eval(data_file).iloc[:, 2:len(\n","                    list(eval(data_file)))] - mean) / std\n","\n","            eval(data_file)['RUL'] = compute_rul_of_one_file(eval(data_file))\n","            current_len = len(eval(data_file))\n","            # print(eval(data_file).index)\n","            eval(data_file).index = range(previous_len, previous_len + current_len)\n","            previous_len = previous_len + current_len\n","            # print(eval(data_file).index)\n","            frames.append(eval(data_file))\n","            print(data_file)\n","\n","        train = pd.concat(frames)\n","        global training_engine_id\n","        training_engine_id = train['engine_id']\n","        train = train.drop('engine_id', 1)\n","        train = train.drop('cycle', 1)\n","        # if files[0] == 1 or files[0] == 3:\n","        #     train = train.drop('setting3', 1)\n","        #     train = train.drop('s18', 1)\n","        #     train = train.drop('s19', 1)\n","\n","        train_values = train.values * SCALE\n","        np.save('normalized_train_data.npy', train_values)\n","        train.to_csv('normalized_train_data.csv')\n","        ###########\n","    else:\n","        train = pd.read_csv('normalized_train_data.csv', index_col=[0])\n","        train_values = train.values\n","\n","    if save_testing_data:  ### testing ###\n","\n","        test_FD001 = pd.read_table(\"./CMAPSSData/test_FD001.txt\", header=None, delim_whitespace=True)\n","        test_FD002 = pd.read_table(\"./CMAPSSData/test_FD002.txt\", header=None, delim_whitespace=True)\n","        test_FD003 = pd.read_table(\"./CMAPSSData/test_FD003.txt\", header=None, delim_whitespace=True)\n","        test_FD004 = pd.read_table(\"./CMAPSSData/test_FD004.txt\", header=None, delim_whitespace=True)\n","        test_FD001.columns = column_name\n","        test_FD002.columns = column_name\n","        test_FD003.columns = column_name\n","        test_FD004.columns = column_name\n","\n","        # load RUL data\n","        RUL_FD001 = pd.read_table(\"./CMAPSSData/RUL_FD001.txt\", header=None, delim_whitespace=True)\n","        RUL_FD002 = pd.read_table(\"./CMAPSSData/RUL_FD002.txt\", header=None, delim_whitespace=True)\n","        RUL_FD003 = pd.read_table(\"./CMAPSSData/RUL_FD003.txt\", header=None, delim_whitespace=True)\n","        RUL_FD004 = pd.read_table(\"./CMAPSSData/RUL_FD004.txt\", header=None, delim_whitespace=True)\n","        RUL_FD001.columns = ['RUL']\n","        RUL_FD002.columns = ['RUL']\n","        RUL_FD003.columns = ['RUL']\n","        RUL_FD004.columns = ['RUL']\n","\n","        previous_len = 0\n","        frames = []\n","        for (data_file, rul_file) in [('test_FD00' + str(i), 'RUL_FD00' + str(i)) for i in files]:\n","            mean = eval(data_file).iloc[:, 2:len(list(eval(data_file)))].mean()\n","            std = eval(data_file).iloc[:, 2:len(list(eval(data_file)))].std()\n","            std.replace(0, 1, inplace=True)\n","\n","            if min_max_norm:\n","                scaler = MinMaxScaler()\n","                eval(data_file).iloc[:, 2:len(list(eval(data_file)))] = scaler.fit_transform(\n","                    eval(data_file).iloc[:, 2:len(list(eval(data_file)))])\n","            else:\n","                eval(data_file).iloc[:, 2:len(list(eval(data_file)))] = (eval(data_file).iloc[:, 2:len(\n","                    list(eval(data_file)))] - mean) / std\n","\n","            eval(data_file)['RUL'] = compute_rul_of_one_file(eval(data_file), RUL_FD00X=eval(rul_file))\n","            current_len = len(eval(data_file))\n","            eval(data_file).index = range(previous_len, previous_len + current_len)\n","            previous_len = previous_len + current_len\n","            frames.append(eval(data_file))\n","            print(data_file)\n","            if len(files) == 1:\n","                global test_engine_id\n","                test_engine_id = eval(data_file)['engine_id']\n","\n","        test = pd.concat(frames)\n","        test = test.drop('engine_id', 1)\n","        test = test.drop('cycle', 1)\n","        # if files[0] == 1 or files[0] == 3:\n","        #     test = test.drop('setting3', 1)\n","        #     test = test.drop('s18', 1)\n","        #     test = test.drop('s19', 1)\n","\n","        test_values = test.values * SCALE\n","        np.save('normalized_test_data.npy', test_values)\n","        test.to_csv('normalized_test_data.csv')\n","        ###########\n","    else:\n","        test = pd.read_csv('normalized_test_data.csv', index_col=[0])\n","        test_values = test.values\n","\n","    return train_values, test_values, train, test\n","\n","\n","def get_PHM08Data(save=False):\n","    \"\"\"\n","    Function is to load PHM 2008 challenge dataset\n","\n","    \"\"\"\n","\n","    if save == False:\n","        return np.load(\"./PHM08/processed_data/phm_training_data.npy\"), np.load(\"./PHM08/processed_data/phm_testing_data.npy\"), np.load(\n","            \"./PHM08/processed_data/phm_original_testing_data.npy\")\n","\n","    column_name = ['engine_id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n","                   's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n","                   's15', 's16', 's17', 's18', 's19', 's20', 's21']\n","    phm_training_data = pd.read_table(\"./PHM08/train.txt\", header=None, delim_whitespace=True)\n","    phm_training_data.columns = column_name\n","    phm_testing_data = pd.read_table(\"./PHM08/final_test.txt\", header=None, delim_whitespace=True)\n","    phm_testing_data.columns = column_name\n","\n","    print(\"phm training\")\n","    mean = phm_training_data.iloc[:, 2:len(list(phm_training_data))].mean()\n","    std = phm_training_data.iloc[:, 2:len(list(phm_training_data))].std()\n","    phm_training_data.iloc[:, 2:len(list(phm_training_data))] = (phm_training_data.iloc[:, 2:len(\n","        list(phm_training_data))] - mean) / std\n","    phm_training_data['RUL'] = compute_rul_of_one_file(phm_training_data)\n","\n","    print(\"phm testing\")\n","    mean = phm_testing_data.iloc[:, 2:len(list(phm_testing_data))].mean()\n","    std = phm_testing_data.iloc[:, 2:len(list(phm_testing_data))].std()\n","    phm_testing_data.iloc[:, 2:len(list(phm_testing_data))] = (phm_testing_data.iloc[:, 2:len(\n","        list(phm_testing_data))] - mean) / std\n","    phm_testing_data['RUL'] = 0\n","    #phm_testing_data['RUL'] = compute_rul_of_one_file(phm_testing_data)\n","\n","    train_engine_id = phm_training_data['engine_id']\n","    # print(phm_training_engine_id[phm_training_engine_id==1].index)\n","    phm_training_data = phm_training_data.drop('engine_id', 1)\n","    phm_training_data = phm_training_data.drop('cycle', 1)\n","\n","    global test_engine_id\n","    test_engine_id = phm_testing_data['engine_id']\n","    phm_testing_data = phm_testing_data.drop('engine_id', 1)\n","    phm_testing_data = phm_testing_data.drop('cycle', 1)\n","\n","    phm_training_data = phm_training_data.values\n","    phm_testing_data = phm_testing_data.values\n","\n","    engine_ids = train_engine_id.unique()\n","    train_test_split = np.random.rand(len(engine_ids)) < 0.80\n","    train_engine_ids = engine_ids[train_test_split]\n","    test_engine_ids = engine_ids[~train_test_split]\n","\n","    # test_engine_id = pd.Series(test_engine_ids)\n","\n","\n","    training_data = phm_training_data[train_engine_id[train_engine_id == train_engine_ids[0]].index]\n","    for id in train_engine_ids[1:]:\n","        tmp = phm_training_data[train_engine_id[train_engine_id == id].index]\n","        training_data = np.concatenate((training_data, tmp))\n","    # print(training_data.shape)\n","\n","    testing_data = phm_training_data[train_engine_id[train_engine_id == test_engine_ids[0]].index]\n","    for id in test_engine_ids[1:]:\n","        tmp = phm_training_data[train_engine_id[train_engine_id == id].index]\n","        testing_data = np.concatenate((testing_data, tmp))\n","    # print(testing_data.shape)\n","\n","    print(phm_training_data.shape, phm_testing_data.shape, training_data.shape, testing_data.shape)\n","\n","    np.save(\"./PHM08/processed_data/phm_training_data.npy\", training_data)\n","    np.savetxt(\"./PHM08/processed_data/phm_training_data.txt\", training_data, delimiter=\" \")\n","    np.save(\"./PHM08/processed_data/phm_testing_data.npy\", testing_data)\n","    np.savetxt(\"./PHM08/processed_data/phm_testing_data.txt\", testing_data, delimiter=\" \")\n","    np.save(\"./PHM08/processed_data/phm_original_testing_data.npy\", phm_testing_data)\n","    np.savetxt(\"./PHM08/processed_data/phm_original_testing_data.csv\", phm_testing_data, delimiter=\",\")\n","\n","    return training_data, testing_data, phm_testing_data\n","\n","\n","def data_augmentation(files=1, low=[10, 40, 90, 170], high=[35, 85, 160, 250], plot=False, combine=False):\n","    '''\n","    This helper function only augments the training data to look like testing data.\n","    Training data always run to a failure. But testing data is mostly stop before a failure.\n","    Therefore, training data augmented to have scenarios without failure\n","\n","    :param files: select wich sub CMPASS dataset\n","    :param low: lower bound for the random selection of the engine cycle\n","    :param high: upper bound for the random selection of the engine cycle\n","    :param plot: switch to plot the augmented data\n","    :return:\n","    '''\n","\n","    DEBUG = False\n","\n","    column_name = ['engine_id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n","                   's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n","                   's15', 's16', 's17', 's18', 's19', 's20', 's21']\n","\n","    ### Loading original data ###\n","    if files == \"phm\":\n","        train_FD00x = pd.read_table(\"./PHM08/processed_data/phm_training_data.txt\", header=None, delim_whitespace=True)\n","        train_FD00x.drop(train_FD00x.columns[len(train_FD00x.columns) - 1], axis=1, inplace=True)\n","        train_FD00x.columns = column_name\n","    else:\n","        if combine:\n","            train_FD00x,_,_ = combine_FD001_and_FD003()\n","        else:\n","            file_path = \"./CMAPSSData/train_FD00\" + str(files) + \".txt\"\n","            train_FD00x = pd.read_table(file_path, header=None, delim_whitespace=True)\n","            train_FD00x.columns = column_name\n","            print(file_path.split(\"/\")[-1])\n","\n","        ### Standered Normal ###\n","        mean = train_FD00x.iloc[:, 2:len(list(train_FD00x))].mean()\n","        std = train_FD00x.iloc[:, 2:len(list(train_FD00x))].std()\n","        std.replace(0, 1, inplace=True)\n","        train_FD00x.iloc[:, 2:len(list(train_FD00x))] = (train_FD00x.iloc[:, 2:len(list(train_FD00x))] - mean) / std\n","\n","    final_train_FD = train_FD00x.copy()\n","    previous_len = 0\n","    frames = []\n","    for i in range(len(high)):\n","        train_FD = train_FD00x.copy()\n","        train_engine_id = train_FD['engine_id']\n","        engine_ids = train_engine_id.unique()\n","        total_ids = len(engine_ids)\n","        train_rul = []\n","        print(\"*************\", final_train_FD.shape, total_ids, low[i], high[i], \"*****************\")\n","\n","        for id in range(1, total_ids + 1):\n","\n","            train_engine_id = train_FD['engine_id']\n","            indexes = train_engine_id[train_engine_id == id].index  ### filter indexes related to id\n","            traj_data = train_FD.loc[indexes]  ### filter trajectory data\n","\n","            cutoff_cycle = random.randint(low[i], high[i])  ### randomly selecting the cutoff point of the engine cycle\n","\n","            if cutoff_cycle > max(traj_data['cycle']):\n","                cutoff_cycle = max(traj_data['cycle'])\n","\n","            train_rul.append(max(traj_data['cycle']) - cutoff_cycle)  ### collecting remaining cycles\n","\n","            cutoff_cycle_index = traj_data['cycle'][traj_data['cycle'] == cutoff_cycle].index  ### cutoff cycle index\n","\n","            if DEBUG:\n","                print(\"traj_shape: \", traj_data.shape, \"current_engine_id:\", id, \"cutoff_cycle:\", cutoff_cycle,\n","                      \"cutoff_index\", cutoff_cycle_index, \"engine_fist_index\", indexes[0], \"engine_last_index\",\n","                      indexes[-1])\n","\n","            ### removing rows after cutoff cycle index ###\n","            if cutoff_cycle_index[0] != indexes[-1]:\n","                drop_range = list(range(cutoff_cycle_index[0] + 1, indexes[-1] + 1))\n","                train_FD.drop(train_FD.index[drop_range], inplace=True)\n","                train_FD.reset_index(drop=True, inplace=True)\n","\n","        ### calculating the RUL for augmented data\n","        train_rul = pd.DataFrame.from_dict({'RUL': train_rul})\n","        train_FD['RUL'] = compute_rul_of_one_file(train_FD, RUL_FD00X=train_rul)\n","\n","        ### changing the engine_id for augmented data\n","        train_engine_id = train_FD['engine_id']\n","        for id in range(1, total_ids + 1):\n","            indexes = train_engine_id[train_engine_id == id].index\n","            train_FD.loc[indexes, 'engine_id'] = id + total_ids * (i + 1)\n","\n","        if i == 0:  # should only execute at the first iteration\n","            final_train_FD['RUL'] = compute_rul_of_one_file(final_train_FD)\n","            current_len = len(final_train_FD)\n","            final_train_FD.index = range(previous_len, previous_len + current_len)\n","            previous_len = previous_len + current_len\n","\n","        ### Re-indexing the augmented data\n","        train_FD['RUL'].index = range(previous_len, previous_len + len(train_FD))\n","        previous_len = previous_len + len(train_FD)\n","\n","        final_train_FD = pd.concat(\n","            [final_train_FD, train_FD])  # concatanete the newly augmented data with previous data\n","\n","    frames.append(final_train_FD)\n","    train = pd.concat(frames)\n","    train.reset_index(drop=True, inplace=True)\n","\n","    train_engine_id = train['engine_id']\n","    # print(train_engine_id)\n","    engine_ids = train_engine_id.unique()\n","    # print(engine_ids[1:])\n","    np.random.shuffle(engine_ids)\n","    # print(engine_ids)\n","\n","    training_data = train.loc[train_engine_id[train_engine_id == engine_ids[0]].index]\n","    training_data.reset_index(drop=True, inplace=True)\n","    previous_len = len(training_data)\n","    for id in engine_ids[1:]:\n","        traj_data = train.loc[train_engine_id[train_engine_id == id].index]\n","        current_len = len(traj_data)\n","        traj_data.index = range(previous_len, previous_len + current_len)\n","        previous_len = previous_len + current_len\n","        training_data = pd.concat([training_data, traj_data])\n","\n","\n","    global training_engine_id\n","    training_engine_id = training_data['engine_id']\n","\n","    training_data = training_data.drop('engine_id', 1)\n","    training_data = training_data.drop('cycle', 1)\n","    # if files == 1 or files == 3:\n","    #     training_data = training_data.drop('setting3', 1)\n","    #     training_data = training_data.drop('s18', 1)\n","    #     training_data = training_data.drop('s19', 1)\n","\n","    training_data_values = training_data.values * SCALE\n","    np.save('normalized_train_data.npy', training_data_values)\n","    training_data.to_csv('normalized_train_data.csv')\n","\n","\n","    train = training_data_values\n","    x_train = train[:, :train.shape[1] - 1]\n","    y_train = train[:, train.shape[1] - 1] * RESCALE\n","    print(\"training in augmentation\", x_train.shape, y_train.shape)\n","\n","    if plot:\n","        plt.plot(y_train, label=\"train\")\n","\n","        plt.figure()\n","        plt.plot(x_train)\n","        plt.title(\"train\")\n","        # plt.figure()\n","        # plt.plot(y_train)\n","        # plt.title(\"test\")\n","\n","        plt.show()\n","\n","\n","def analyse_Data(dataset, files=None, plot=True, min_max=False):\n","    '''\n","    Generate pre-processed data according to the given dataset\n","    :param dataset: choose between \"phm\" for PHM 2008 dataset or \"cmapss\" for CMAPSS data set with file number\n","    :param files: Only for CMAPSS dataset to select sub dataset\n","    :param min_max: switch to allow min-max normalization\n","    :return:\n","    '''\n","\n","    if dataset == \"phm\":\n","        training_data, testing_data, phm_testing_data = get_PHM08Data(save=True)\n","\n","        x_phmtrain = training_data[:, :training_data.shape[1] - 1]\n","        y_phmtrain = training_data[:, training_data.shape[1] - 1]\n","\n","        x_phmtest = testing_data[:, :testing_data.shape[1] - 1]\n","        y_phmtest = testing_data[:, testing_data.shape[1] - 1]\n","\n","        print(\"phmtrain\", x_phmtrain.shape, y_phmtrain.shape)\n","\n","        print(\"phmtest\", x_phmtrain.shape, y_phmtrain.shape)\n","        print(\"phmtest\", phm_testing_data.shape)\n","\n","        if plot:\n","            # plt.plot(x_phmtrain, label=\"phmtrain_x\")\n","            plt.figure()\n","            plt.plot(y_phmtrain, label=\"phmtrain_y\")\n","\n","            # plt.figure()\n","            # plt.plot(x_phmtest, label=\"phmtest_x\")\n","            plt.figure()\n","            plt.plot(y_phmtest, label=\"phmtest_y\")\n","\n","            # plt.figure()\n","            # plt.plot(phm_testing_data, label=\"test\")\n","            plt.show()\n","\n","    elif dataset == \"cmapss\":\n","        training_data, testing_data, training_pd, testing_pd = get_CMAPSSData(save=True, files=files,\n","                                                                              min_max_norm=min_max)\n","        x_train = training_data[:, :training_data.shape[1] - 1]\n","        y_train = training_data[:, training_data.shape[1] - 1]\n","        print(\"training\", x_train.shape, y_train.shape)\n","\n","        x_test = testing_data[:, :testing_data.shape[1] - 1]\n","        y_test = testing_data[:, testing_data.shape[1] - 1]\n","        print(\"testing\", x_test.shape, y_test.shape)\n","\n","        if plot:\n","            plt.plot(y_train, label=\"train\")\n","            plt.figure()\n","            plt.plot(y_test, label=\"test\")\n","\n","            plt.figure()\n","            plt.plot(x_train)\n","            plt.title(\"train: FD00\" + str(files[0]))\n","            plt.figure()\n","            plt.plot(y_train)\n","            plt.title(\"train: FD00\" + str(files[0]))\n","            plt.show()\n","\n","\n","def combine_FD001_and_FD003():\n","    column_name = ['engine_id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n","                   's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n","                   's15', 's16', 's17', 's18', 's19', 's20', 's21']\n","\n","    train_FD001 = pd.read_table(\"./CMAPSSData/train_FD001.txt\", header=None, delim_whitespace=True)\n","    train_FD003 = pd.read_table(\"./CMAPSSData/train_FD003.txt\", header=None, delim_whitespace=True)\n","    train_FD001.columns = column_name\n","    train_FD003.columns = column_name\n","\n","    FD001_max_engine_id = max(train_FD001['engine_id'])\n","    train_FD003['engine_id'] = train_FD003['engine_id'] + FD001_max_engine_id\n","    train_FD003.index = range(len(train_FD001), len(train_FD001) + len(train_FD003))\n","    train_FD001_FD002 = pd.concat([train_FD001,train_FD003])\n","\n","    test_FD001 = pd.read_table(\"./CMAPSSData/test_FD001.txt\", header=None, delim_whitespace=True)\n","    test_FD003 = pd.read_table(\"./CMAPSSData/test_FD003.txt\", header=None, delim_whitespace=True)\n","    test_FD001.columns = column_name\n","    test_FD003.columns = column_name\n","\n","    FD001_max_engine_id = max(test_FD001['engine_id'])\n","    test_FD003['engine_id'] = test_FD003['engine_id'] + FD001_max_engine_id\n","    test_FD003.index = range(len(test_FD001), len(test_FD001) + len(test_FD003))\n","    test_FD001_FD002 = pd.concat([test_FD001,test_FD003])\n","\n","    RUL_FD001 = pd.read_table(\"./CMAPSSData/RUL_FD001.txt\", header=None, delim_whitespace=True)\n","    RUL_FD003 = pd.read_table(\"./CMAPSSData/RUL_FD003.txt\", header=None, delim_whitespace=True)\n","    RUL_FD001.columns = ['RUL']\n","    RUL_FD003.columns = ['RUL']\n","    RUL_FD003.index = range(len(RUL_FD001), len(RUL_FD001) + len(RUL_FD003))\n","    RUL_FD001_FD002 = pd.concat([test_FD001, test_FD003])\n","\n","    return train_FD001_FD002,test_FD001_FD002,RUL_FD001_FD002\n"],"metadata":{"id":"ZBFZdeO43jtA","executionInfo":{"status":"ok","timestamp":1643956592591,"user_tz":480,"elapsed":1855,"user":{"displayName":"Yoshifumi Suzuki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhA5ySxPD4d5EAxHB7_YxB1GzsFPXBv17bpN1w-uUA=s64","userId":"11502972587532747475"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["## model.py"],"metadata":{"id":"mGO4Nm4x3t_X"}},{"cell_type":"code","source":["from matplotlib import pyplot as plt\n","import time\n","import datetime\n","# from utils_laj import *\n","# from data_processing import get_CMAPSSData, get_PHM08Data, data_augmentation, analyse_Data\n","\n","today = datetime.date.today()"],"metadata":{"id":"mrkEOs7S4b6Y","executionInfo":{"status":"ok","timestamp":1643956599104,"user_tz":480,"elapsed":234,"user":{"displayName":"Yoshifumi Suzuki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhA5ySxPD4d5EAxHB7_YxB1GzsFPXBv17bpN1w-uUA=s64","userId":"11502972587532747475"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def CNNLSTM(dataset, file_no, Train=False, trj_wise=False, plot=False):\n","    '''\n","    The architecture is a Meny-to-meny model combining CNN and LSTM models\n","    :param dataset: select the specific dataset between PHM08 or CMAPSS\n","    :param Train: select between training and testing\n","    :param trj_wise: Trajectorywise calculate RMSE and scores\n","    '''\n","\n","    #### checkpoint saving path ####\n","    if file_no == 1:\n","        path_checkpoint = './Save/Save_CNNLSTM/CNNLSTM_ML120_GRAD1_kinkRUL_FD001/CNN1D_3_lstm_2_layers'\n","    elif file_no == 2:\n","        path_checkpoint = './Save/Save_CNNLSTM/CNNLSTM_ML120_GRAD1_kinkRUL_FD002/CNN1D_3_lstm_2_layers'\n","    elif file_no == 3:\n","        path_checkpoint = './Save/Save_CNNLSTM/CNNLSTM_ML120_GRAD1_kinkRUL_FD003/CNN1D_3_lstm_2_layers'\n","    elif file_no == 4:\n","        path_checkpoint = './Save/Save_CNNLSTM/CNNLSTM_ML120_GRAD1_kinkRUL_FD004/CNN1D_3_lstm_2_layers'\n","    else:\n","        raise ValueError(\"Save path not defined\")\n","    ##################################\n","\n","\n","    if dataset == \"cmapss\":\n","        training_data, testing_data, training_pd, testing_pd = get_CMAPSSData(save=False)\n","        x_train = training_data[:, :training_data.shape[1] - 1]\n","        y_train = training_data[:, training_data.shape[1] - 1]\n","        print(\"training data CNNLSTM: \", x_train.shape, y_train.shape)\n","\n","        x_test = testing_data[:, :testing_data.shape[1] - 1]\n","        y_test = testing_data[:, testing_data.shape[1] - 1]\n","        print(\"testing data CNNLSTM: \", x_test.shape, y_test.shape)\n","\n","    elif dataset == \"phm\":\n","        training_data, testing_data, phm_testing_data = get_PHM08Data(save=False)\n","        x_validation = phm_testing_data[:, :phm_testing_data.shape[1] - 1]\n","        y_validation = phm_testing_data[:, phm_testing_data.shape[1] - 1]\n","        print(\"testing data: \", x_validation.shape, y_validation.shape)\n","\n","    batch_size = 1024  # Batch size\n","    if Train == False: batch_size = 5\n","\n","    sequence_length = 100  # Number of steps\n","    learning_rate = 0.001  # 0.0001\n","    # epochs = 5000\n","    epochs = 10\n","\n","    ann_hidden = 50\n","\n","    n_channels = 24\n","\n","    lstm_size = n_channels * 3  # 3 times the amount of channels\n","    num_layers = 2  # 2  # Number of layers\n","\n","    X = tf.placeholder(tf.float32, [None, sequence_length, n_channels], name='inputs')\n","    Y = tf.placeholder(tf.float32, [None, sequence_length], name='labels')\n","    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n","    learning_rate_ = tf.placeholder(tf.float32, name='learning_rate')\n","    is_train = tf.placeholder(dtype=tf.bool, shape=None, name=\"is_train\")\n","\n","    conv1 = conv_layer(X, filters=18, kernel_size=2, strides=1, padding='same', batch_norm=False, is_train=is_train,\n","                       scope='conv_1')\n","    max_pool_1 = tf.layers.max_pooling1d(inputs=conv1, pool_size=2, strides=2, padding='same', name='maxpool_1')\n","\n","    conv2 = conv_layer(max_pool_1, filters=36, kernel_size=2, strides=1, padding='same', batch_norm=False,\n","                       is_train=is_train, scope='conv_2')\n","    max_pool_2 = tf.layers.max_pooling1d(inputs=conv2, pool_size=2, strides=2, padding='same', name='maxpool_2')\n","\n","    conv3 = conv_layer(max_pool_2, filters=72, kernel_size=2, strides=1, padding='same', batch_norm=False,\n","                       is_train=is_train, scope='conv_3')\n","    max_pool_3 = tf.layers.max_pooling1d(inputs=conv3, pool_size=2, strides=2, padding='same', name='maxpool_3')\n","\n","    conv_last_layer = max_pool_3\n","\n","    shape = conv_last_layer.get_shape().as_list()\n","    CNN_flat = tf.reshape(conv_last_layer, [-1, shape[1] * shape[2]])\n","\n","    dence_layer_1 = dense_layer(CNN_flat, size=sequence_length * n_channels, activation_fn=tf.nn.relu, batch_norm=False,\n","                                phase=is_train, drop_out=True, keep_prob=keep_prob,\n","                                scope=\"fc_1\")\n","    lstm_input = tf.reshape(dence_layer_1, [-1, sequence_length, n_channels])\n","\n","    cell = get_RNNCell(['LSTM'] * num_layers, keep_prob=keep_prob, state_size=lstm_size)\n","    init_state = cell.zero_state(batch_size, tf.float32)\n","    rnn_output, states = tf.nn.dynamic_rnn(cell, lstm_input, dtype=tf.float32, initial_state=init_state)\n","    stacked_rnn_output = tf.reshape(rnn_output, [-1, lstm_size])  # change the form into a tensor\n","\n","    dence_layer_2 = dense_layer(stacked_rnn_output, size=ann_hidden, activation_fn=tf.nn.relu, batch_norm=False,\n","                                phase=is_train, drop_out=True, keep_prob=keep_prob,\n","                                scope=\"fc_2\")\n","\n","    output = dense_layer(dence_layer_2, size=1, activation_fn=None, batch_norm=False, phase=is_train, drop_out=False,\n","                         keep_prob=keep_prob,\n","                         scope=\"fc_3_output\")\n","\n","    prediction = tf.reshape(output, [-1])\n","    y_flat = tf.reshape(Y, [-1])\n","\n","    h = prediction - y_flat\n","\n","    cost_function = tf.reduce_sum(tf.square(h))\n","    RMSE = tf.sqrt(tf.reduce_mean(tf.square(h)))\n","    optimizer = tf.train.AdamOptimizer(learning_rate_).minimize(cost_function)\n","\n","    saver = tf.train.Saver()\n","    training_generator = batch_generator(x_train, y_train, batch_size, sequence_length, online=True)\n","    testing_generator = batch_generator(x_test, y_test, batch_size, sequence_length, online=False)\n","\n","    if Train: model_summary(learning_rate=learning_rate, batch_size=batch_size, lstm_layers=num_layers,\n","                            lstm_layer_size=lstm_size, fc_layer_size=ann_hidden, sequence_length=sequence_length,\n","                            n_channels=n_channels, path_checkpoint=path_checkpoint, spacial_note='')\n","\n","    with tf.Session() as session:\n","        tf.global_variables_initializer().run()\n","\n","        if Train == True:\n","            cost = []\n","            iteration = int(x_train.shape[0] / batch_size)\n","            print(\"Training set MSE\")\n","            print(\"No epoches: \", epochs, \"No itr: \", iteration)\n","            __start = time.time()\n","            for ep in range(epochs):\n","\n","                for itr in range(iteration):\n","                    ## training ##\n","                    batch_x, batch_y = next(training_generator)\n","                    session.run(optimizer,\n","                                feed_dict={X: batch_x, Y: batch_y, keep_prob: 0.8, learning_rate_: learning_rate})\n","                    cost.append(\n","                        RMSE.eval(feed_dict={X: batch_x, Y: batch_y, keep_prob: 1.0, learning_rate_: learning_rate}))\n","\n","                x_test_batch, y_test_batch = next(testing_generator)\n","                mse_train, rmse_train = session.run([cost_function, RMSE],\n","                                                    feed_dict={X: batch_x, Y: batch_y, keep_prob: 1.0,\n","                                                               learning_rate_: learning_rate})\n","                mse_test, rmse_test = session.run([cost_function, RMSE],\n","                                                  feed_dict={X: x_test_batch, Y: y_test_batch, keep_prob: 1.0,\n","                                                             learning_rate_: learning_rate})\n","\n","                time_per_ep = (time.time() - __start)\n","                time_remaining = ((epochs - ep) * time_per_ep) / 3600\n","                print(\"CNNLSTM\", \"epoch:\", ep, \"\\tTrainig-\",\n","                      \"MSE:\", mse_train, \"RMSE:\", rmse_train, \"\\tTesting-\", \"MSE\", mse_test, \"RMSE\", rmse_test,\n","                      \"\\ttime/epoch:\", round(time_per_ep, 2), \"\\ttime_remaining: \",\n","                      int(time_remaining), \" hr:\", round((time_remaining % 1) * 60, 1), \" min\", \"\\ttime_stamp: \",\n","                      datetime.datetime.now().strftime(\"%Y.%m.%d-%H:%M:%S\"))\n","                __start = time.time()\n","\n","                if ep % 10 == 0 and ep != 0:\n","                    save_path = saver.save(session, path_checkpoint)\n","                    if os.path.exists(path_checkpoint + '.meta'):\n","                        print(\"Model saved to file: %s\" % path_checkpoint)\n","                    else:\n","                        print(\"NOT SAVED!!!\", path_checkpoint)\n","\n","                if ep % 1000 == 0 and ep != 0: learning_rate = learning_rate / 10\n","\n","            save_path = saver.save(session, path_checkpoint)\n","            if os.path.exists(path_checkpoint + '.meta'):\n","                print(\"Model saved to file: %s\" % path_checkpoint)\n","            else:\n","                print(\"NOT SAVED!!!\", path_checkpoint)\n","            plt.plot(cost)\n","            plt.show()\n","        else:\n","            saver.restore(session, path_checkpoint)\n","            print(\"Model restored from file: %s\" % path_checkpoint)\n","\n","            if trj_wise:\n","                trj_iteration = len(test_engine_id.unique())\n","                print(\"total trajectories: \", trj_iteration)\n","                error_list = []\n","                pred_list = []\n","                expected_list = []\n","                lower_bound = -0.01\n","                test_trjectory_generator = trjectory_generator(x_test, y_test, test_engine_id, sequence_length,\n","                                                               batch_size, lower_bound)\n","                for itr in range(trj_iteration):\n","                    trj_x, trj_y = next(test_trjectory_generator)\n","\n","                    __y_pred, error, __y = session.run([prediction, h, y_flat],\n","                                                       feed_dict={X: trj_x, Y: trj_y, keep_prob: 1.0})\n","\n","                    RUL_predict, RUL_expected = get_predicted_expected_RUL(__y, __y_pred, lower_bound)\n","\n","                    error_list.append(RUL_predict - RUL_expected)\n","                    pred_list.append(RUL_predict)\n","                    expected_list.append(RUL_expected)\n","\n","                    print(\"id: \", itr + 1, \"expected: \", RUL_expected, \"\\t\", \"predict: \", RUL_predict, \"\\t\", \"error: \",\n","                          RUL_predict - RUL_expected)\n","                    # plt.plot(__y_pred* RESCALE, label=\"prediction\")\n","                    # plt.plot(__y* RESCALE, label=\"expected\")\n","                    # plt.show()\n","                error_list = np.array(error_list)\n","                error_list = error_list.ravel()\n","                rmse = np.sqrt(np.sum(np.square(error_list)) / len(error_list))  # RMSE\n","                print(rmse, scoring_func(error_list))\n","                if plot:\n","                    plt.figure()\n","                    # plt.plot(expected_list, 'o', color='black', label=\"expected\")\n","                    # plt.plot(pred_list, 'o', color='red', label=\"predicted\")\n","                    # plt.figure()\n","                    plt.plot(np.sort(error_list), 'o', color='red', label=\"error\")\n","                    plt.legend()\n","                    plt.show()\n","                fig, ax = plt.subplots()\n","                ax.stem(expected_list, linefmt='b-', label=\"expected\")\n","                ax.stem(pred_list, linefmt='r-', label=\"predicted\")\n","                plt.legend()\n","                plt.show()\n","\n","            else:\n","                x_validation = x_test\n","                y_validation = y_test\n","\n","                validation_generator = batch_generator(x_validation, y_validation, batch_size, sequence_length,\n","                                                       online=True,\n","                                                       online_shift=sequence_length)\n","\n","                full_prediction = []\n","                actual_rul = []\n","                error_list = []\n","\n","                iteration = int(x_validation.shape[0] / (batch_size * sequence_length))\n","                print(\"#of validation points:\", x_validation.shape[0], \"#datapoints covers from minibatch:\",\n","                      batch_size * sequence_length, \"iterations/epoch\", iteration)\n","\n","                for itr in range(iteration):\n","                    x_validate_batch, y_validate_batch = next(validation_generator)\n","                    __y_pred, error, __y = session.run([prediction, h, y_flat],\n","                                                       feed_dict={X: x_validate_batch, Y: y_validate_batch,\n","                                                                  keep_prob: 1.0})\n","                    full_prediction.append(__y_pred * RESCALE)\n","                    actual_rul.append(__y * RESCALE)\n","                    error_list.append(error * RESCALE)\n","                full_prediction = np.array(full_prediction)\n","                full_prediction = full_prediction.ravel()\n","                actual_rul = np.array(actual_rul)\n","                actual_rul = actual_rul.ravel()\n","                error_list = np.array(error_list)\n","                error_list = error_list.ravel()\n","                rmse = np.sqrt(np.sum(np.square(error_list)) / len(error_list))  # RMSE\n","\n","                print(y_validation.shape, full_prediction.shape, \"RMSE:\", rmse, \"Score:\", scoring_func(error_list))\n","                if plot:\n","                    plt.plot(full_prediction, label=\"prediction\")\n","                    plt.plot(actual_rul, label=\"expected\")\n","                    plt.legend()\n","                    plt.show()"],"metadata":{"id":"4Rm5uxwW4NUE","executionInfo":{"status":"ok","timestamp":1643956601006,"user_tz":480,"elapsed":562,"user":{"displayName":"Yoshifumi Suzuki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhA5ySxPD4d5EAxHB7_YxB1GzsFPXBv17bpN1w-uUA=s64","userId":"11502972587532747475"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","\n","    dataset = \"cmapss\" \n","    file = 4 # represent the sub-dataset for cmapss\n","    TRAIN = True\n","    # TRAIN = False\n","    \n","    TRJ_WISE = True\n","    PLOT = True\n","\n","    analyse_Data(dataset=dataset, files=[file], plot=False, min_max=False)\n","\n","    if TRAIN: data_augmentation(files=file,\n","                                low=[10, 35, 50, 70, 90, 110, 130, 150, 170, 190, 210, 230, 250, 270, 290, 310, 330],\n","                                high=[35, 50, 70, 90, 110, 130, 150, 170, 190, 210, 230, 250, 270, 290, 310, 330, 350],\n","                                plot=False,\n","                                combine=False)\n","\n","    # from data_processing import RESCALE, test_engine_id\n","\n","    CNNLSTM(dataset=dataset, file_no=file, Train=TRAIN, trj_wise=TRJ_WISE, plot=PLOT)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Nb4NIj54hvA","outputId":"e5c63d09-a4f8-4f6a-8e64-3c7bafc802cd"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["train_FD004\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:125: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:126: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["test_FD004\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:187: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:188: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["training (61249, 24) (61249,)\n","testing (41214, 24) (41214,)\n","train_FD004.txt\n","************* (61249, 26) 249 10 35 *****************\n","************* (66746, 27) 249 35 50 *****************\n","************* (77304, 27) 249 50 70 *****************\n","************* (92194, 27) 249 70 90 *****************\n","************* (112057, 27) 249 90 110 *****************\n","************* (136928, 27) 249 110 130 *****************\n","************* (166814, 27) 249 130 150 *****************\n","************* (201702, 27) 249 150 170 *****************\n","************* (241374, 27) 249 170 190 *****************\n","************* (285196, 27) 249 190 210 *****************\n","************* (332784, 27) 249 210 230 *****************\n","************* (383400, 27) 249 230 250 *****************\n","************* (436593, 27) 249 250 270 *****************\n","************* (491849, 27) 249 270 290 *****************\n","************* (548933, 27) 249 290 310 *****************\n","************* (607207, 27) 249 310 330 *****************\n","************* (666383, 27) 249 330 350 *****************\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:407: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:408: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n"]},{"output_type":"stream","name":"stdout","text":["training in augmentation (726236, 24) (726236,)\n","training data CNNLSTM:  (726236, 24) (726236,)\n","testing data CNNLSTM:  (41214, 24) (41214,)\n","WARNING:tensorflow:From <ipython-input-10-815ed1e420cf>:286: conv1d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.keras.layers.Conv1D` instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/layers/convolutional.py:218: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.__call__` method instead.\n","WARNING:tensorflow:From <ipython-input-14-b9ced71a484d>:62: max_pooling1d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.MaxPooling1D instead.\n","WARNING:tensorflow:From <ipython-input-10-815ed1e420cf>:23: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.Dense instead.\n","WARNING:tensorflow:From <ipython-input-10-815ed1e420cf>:25: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From <ipython-input-10-815ed1e420cf>:46: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:From <ipython-input-10-815ed1e420cf>:60: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:From <ipython-input-14-b9ced71a484d>:84: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.add_weight` method instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","Training set MSE\n","No epoches:  10 No itr:  709\n","CNNLSTM epoch: 0 \tTrainig- MSE: 154149980.0 RMSE: 38.79911 \tTesting- MSE 72757630.0 RMSE 26.655651 \ttime/epoch: 1736.95 \ttime_remaining:  4  hr: 49.5  min \ttime_stamp:  2022.02.04-07:08:33\n","CNNLSTM epoch: 1 \tTrainig- MSE: 152961470.0 RMSE: 38.64925 \tTesting- MSE 76935864.0 RMSE 27.410341 \ttime/epoch: 1705.03 \ttime_remaining:  4  hr: 15.8  min \ttime_stamp:  2022.02.04-07:36:58\n","CNNLSTM epoch: 2 \tTrainig- MSE: 24183314.0 RMSE: 15.367666 \tTesting- MSE 48067650.0 RMSE 21.665886 \ttime/epoch: 1702.66 \ttime_remaining:  3  hr: 47.0  min \ttime_stamp:  2022.02.04-08:05:21\n"]}]}]}